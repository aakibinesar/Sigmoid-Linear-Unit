# Sigmoid-Linear-Unit-Project
Sigmoid Linear Unit (SiLU), a high performing neural network activation function and a variation of Gaussian Error Linear Unit (GELU), was implemented in Deep Neural Network and Convolutional Neural Network.

This project was motivated from the paper: Hendrycks, Dan, and Kevin Gimpel. "Gaussian error linear units (gelus)." arXiv preprint arXiv: 1606.08415 (2016). (https://arxiv.org/pdf/1606.08415.pdf)

For DNN implementation, check: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1r1nHJ3lqaWbuynpKZDMQvNLnJkD_BVlr)

For CNN implementation, check: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1EMq3fgEM714dinUbATZVWiaphBJQTOvZ)
